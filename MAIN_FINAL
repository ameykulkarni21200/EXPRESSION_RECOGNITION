import cv2
import numpy as np
from deepface import DeepFace
from collections import deque
import time

# Load the trained model for emotion detection
model.load_weights('facial_expression_model.h5')

# Initialize the webcam for real-time video capture
cap = cv2.VideoCapture(0)

# Track face data for each detected face
face_data = {}

# Helper function to calculate the Euclidean distance between two points
def calculate_distance(box1, box2):
    x1, y1 = box1[:2]
    x2, y2 = box2[:2]
    return np.sqrt((x1 - x2)**2 + (y1 - y2)**2)

while True:
    ret, frame = cap.read()
    if not ret:
        break

    # Detect faces in the frame using DeepFace with the 'mtcnn' backend
    faces = DeepFace.extract_faces(frame, detector_backend='mtcnn', enforce_detection=False)

    # Check if any faces were detected
    if faces:
        new_face_data = {}
        for face in faces:
            # Extract bounding box coordinates
            x, y, w, h = (face['facial_area']['x'], face['facial_area']['y'], 
                          face['facial_area']['w'], face['facial_area']['h'])
            box = (x, y, w, h)

            # Match detected face with existing faces in face_data using proximity
            matched_id = None
            for face_id, data in face_data.items():
                if calculate_distance(box, data['last_position']) < 50:  # Threshold for matching
                    matched_id = face_id
                    break

            # Create new face ID if no match found
            if matched_id is None:
                matched_id = f"{x}_{y}_{w}_{h}"
                face_data[matched_id] = {
                    'fear_start_time': None,
                    'alert_raised': False,
                    'emotion_history': deque(maxlen=30),
                    'last_position': box
                }

            # Update last position of the face
            face_data[matched_id]['last_position'] = box

            # Extract and preprocess the face image
            face_img = frame[y:y+h, x:x+w]
            face_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2GRAY)
            face_img = cv2.resize(face_img, (48, 48))
            face_img = face_img.reshape(1, 48, 48, 1) / 255.0

            # Predict emotion and map it to a label
            prediction = model.predict(face_img)
            emotion = np.argmax(prediction)
            emotion_text = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral'][emotion]
            face_data[matched_id]['emotion_history'].append(emotion_text)

            # Draw bounding box and emotion label
            color = (255, 0, 0)  # Default color for face box
            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)
            cv2.putText(frame, emotion_text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)

            # Check if 'Fear' emotion is detected
            if emotion_text == 'Fear':
                if face_data[matched_id]['fear_start_time'] is None:
                    face_data[matched_id]['fear_start_time'] = time.time()
                elif time.time() - face_data[matched_id]['fear_start_time'] >= 1 and not face_data[matched_id]['alert_raised']:
                    # Raise alert after 1 second of continuous 'Fear' expression
                    face_data[matched_id]['alert_raised'] = True
                    suspicious_face_image = frame[y:y+h, x:x+w]
                    cv2.imwrite(f'suspicious_face_{matched_id}.png', suspicious_face_image)
            else:
                # Reset fear tracking if 'Fear' is not detected
                face_data[matched_id]['fear_start_time'] = None
                face_data[matched_id]['alert_raised'] = False

            # Highlight and display alert for faces with continuous 'Fear'
            if face_data[matched_id]['alert_raised']:
                color = (0, 0, 255)  # Red color for alert-raised face
                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)
                cv2.putText(frame, 'Alert: Suspicious Person Detected', 
                            (x, y - 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

            # Update new_face_data to keep track of active faces
            new_face_data[matched_id] = face_data[matched_id]

        # Keep only the active faces in face_data for tracking
        face_data = new_face_data

    # Display the frame
    cv2.imshow('Video', frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
